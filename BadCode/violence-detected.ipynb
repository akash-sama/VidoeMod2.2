{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 397693,
     "sourceType": "datasetVersion",
     "datasetId": 176381
    },
    {
     "sourceId": 7971919,
     "sourceType": "datasetVersion",
     "datasetId": 4690902
    }
   ],
   "dockerImageVersionId": 30673,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Cell 1: Importing necessary libraries\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, BatchNormalization, Dropout, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-03-29T08:43:41.619714Z",
     "iopub.execute_input": "2024-03-29T08:43:41.621045Z",
     "iopub.status.idle": "2024-03-29T08:43:43.354786Z",
     "shell.execute_reply.started": "2024-03-29T08:43:41.620874Z",
     "shell.execute_reply": "2024-03-29T08:43:43.353672Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def extract_frame_features(frame, pretrained_model):\n",
    "    # Expand the dimensions of the frame for model compatibility\n",
    "    img = np.expand_dims(frame, axis=0)\n",
    "    # Use the pre-trained feature extraction model to obtain the feature vector\n",
    "    feature_vector = pretrained_model.predict(img, verbose=0)\n",
    "    # Return the extracted feature vector\n",
    "    return feature_vector\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T08:41:38.918743Z",
     "iopub.execute_input": "2024-03-29T08:41:38.919271Z",
     "iopub.status.idle": "2024-03-29T08:41:38.925712Z",
     "shell.execute_reply.started": "2024-03-29T08:41:38.919229Z",
     "shell.execute_reply": "2024-03-29T08:41:38.924325Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def extract_video_frames(video_path, sequence_length=16, image_width=299, image_height=299):\n",
    "    frames_list = []\n",
    "    # Open the video file for reading\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    # Get the total number of frames in the video\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # Calculate the number of frames to skip in order to achieve the desired sequence length\n",
    "    skip_frames_window = max(int(video_frames_count / sequence_length), 1)\n",
    "\n",
    "    # Loop through each frame in the sequence\n",
    "    for frame_counter in range(sequence_length):\n",
    "        # Set the position of the video reader to the current frame\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "        # Read the frame\n",
    "        success, frame = video_reader.read()\n",
    "        # Break if unable to read the frame\n",
    "        if not success:\n",
    "            break\n",
    "        # Convert the frame to RGB and resize it\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        resized_frame = cv2.resize(frame_rgb, (image_height, image_width))\n",
    "        # Append the resized frame to the frames list\n",
    "        frames_list.append(resized_frame)\n",
    "    \n",
    "    # Release the video reader\n",
    "    video_reader.release()\n",
    "    # Return the list of frames\n",
    "    return frames_list\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T08:41:46.867632Z",
     "iopub.execute_input": "2024-03-29T08:41:46.868788Z",
     "iopub.status.idle": "2024-03-29T08:41:46.877591Z",
     "shell.execute_reply.started": "2024-03-29T08:41:46.868732Z",
     "shell.execute_reply": "2024-03-29T08:41:46.876267Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def extract_features_from_videos(video_paths, total_videos, pretrained_model):\n",
    "    all_video_features = []\n",
    "    # Loop through each video\n",
    "    for pos in tqdm(range(total_videos)):\n",
    "        frames_list = []\n",
    "        # Extract frames from the current video\n",
    "        frames = extract_video_frames(video_paths[pos])\n",
    "        # Extract features from each frame\n",
    "        for frame in frames:\n",
    "            features = extract_frame_features(frame, pretrained_model)\n",
    "            frames_list.append(features)\n",
    "        all_video_features.append(frames_list)\n",
    "    return np.array(all_video_features)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T08:41:56.292583Z",
     "iopub.execute_input": "2024-03-29T08:41:56.294307Z",
     "iopub.status.idle": "2024-03-29T08:41:56.300421Z",
     "shell.execute_reply.started": "2024-03-29T08:41:56.294258Z",
     "shell.execute_reply": "2024-03-29T08:41:56.299295Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define violence and non-violence directories\n",
    "import os\n",
    "violence_dir = '/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence'\n",
    "nonviolence_dir = '/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence'\n",
    "\n",
    "# Create paths to individual videos\n",
    "violence_path = [os.path.join(violence_dir, name) for name in os.listdir(violence_dir)]\n",
    "nonviolence_path = [os.path.join(nonviolence_dir, name) for name in os.listdir(nonviolence_dir)]\n",
    "\n",
    "# Extract features from videos\n",
    "violence_features = extract_features_from_videos(violence_path[:500], len(violence_path[:500]), pretrained_model)\n",
    "non_violence_features = extract_features_from_videos(nonviolence_path[:500], len(nonviolence_path[:500]), pretrained_model)\n",
    "\n",
    "# Save extracted features\n",
    "\n",
    "np.save('/kaggle/working/violence_features.npy', violence_features)\n",
    "np.save('/kaggle/working/non_violence_features.npy', non_violence_features)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T08:43:50.270838Z",
     "iopub.execute_input": "2024-03-29T08:43:50.271248Z",
     "iopub.status.idle": "2024-03-29T09:49:05.574251Z",
     "shell.execute_reply.started": "2024-03-29T08:43:50.271214Z",
     "shell.execute_reply": "2024-03-29T09:49:05.573014Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 500/500 [35:00<00:00,  4.20s/it]\n 99%|█████████▉| 497/500 [30:00<00:13,  4.37s/it][h264 @ 0x5c5d1e0a1a40] mb_type 104 in P slice too large at 98 31\n[h264 @ 0x5c5d1e0a1a40] error while decoding MB 98 31\n[h264 @ 0x5c5d1e0a1a40] mb_type 104 in P slice too large at 98 31\n[h264 @ 0x5c5d1e0a1a40] error while decoding MB 98 31\n[h264 @ 0x5c5d1e0a1a40] mb_type 104 in P slice too large at 98 31\n[h264 @ 0x5c5d1e0a1a40] error while decoding MB 98 31\n[h264 @ 0x5c5d1e0a1a40] mb_type 104 in P slice too large at 98 31\n[h264 @ 0x5c5d1e0a1a40] error while decoding MB 98 31\n100%|██████████| 500/500 [30:14<00:00,  3.63s/it]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load features and labels\n",
    "violence_features = np.load('/kaggle/working/violence_features.npy')\n",
    "non_violence_features = np.load('/kaggle/working/non_violence_features.npy')\n",
    "\n",
    "# Creating labels\n",
    "violence_labels = np.zeros(len(violence_features))\n",
    "non_violence_labels = np.ones(len(non_violence_features))\n",
    "\n",
    "# Combining features and labels\n",
    "X = np.concatenate([violence_features, non_violence_features], axis=0)\n",
    "y = np.concatenate([violence_labels, non_violence_labels], axis=0)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "# Reshaping data for LSTM input\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 16, 2048))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 16, 2048))\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T09:52:36.174404Z",
     "iopub.execute_input": "2024-03-29T09:52:36.174826Z",
     "iopub.status.idle": "2024-03-29T09:52:36.360607Z",
     "shell.execute_reply.started": "2024-03-29T09:52:36.174790Z",
     "shell.execute_reply": "2024-03-29T09:52:36.359479Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "inputs = Input(shape=(16, 2048))\n",
    "x = Bidirectional(LSTM(200, return_sequences=True))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Bidirectional(LSTM(100))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_reshaped, y_train, validation_data=(X_test_reshaped, y_test), epochs=5, batch_size=32)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T09:52:45.955594Z",
     "iopub.execute_input": "2024-03-29T09:52:45.956656Z",
     "iopub.status.idle": "2024-03-29T09:53:16.258288Z",
     "shell.execute_reply.started": "2024-03-29T09:52:45.956608Z",
     "shell.execute_reply": "2024-03-29T09:53:16.257163Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1mModel: \"functional_5\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_4 (\u001B[38;5;33mInputLayer\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m, \u001B[38;5;34m2048\u001B[0m)       │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional (\u001B[38;5;33mBidirectional\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m, \u001B[38;5;34m400\u001B[0m)        │     \u001B[38;5;34m3,598,400\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_376         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m, \u001B[38;5;34m400\u001B[0m)        │         \u001B[38;5;34m1,600\u001B[0m │\n│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m, \u001B[38;5;34m400\u001B[0m)        │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_1 (\u001B[38;5;33mBidirectional\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m)            │       \u001B[38;5;34m400,800\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_377         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m)            │           \u001B[38;5;34m800\u001B[0m │\n│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m)            │        \u001B[38;5;34m40,200\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │           \u001B[38;5;34m201\u001B[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,598,400</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_376         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">400,800</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_377         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,200</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m4,042,001\u001B[0m (15.42 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,042,001</span> (15.42 MB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m4,040,801\u001B[0m (15.41 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,040,801</span> (15.41 MB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m1,200\u001B[0m (4.69 KB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> (4.69 KB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Epoch 1/5\n\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 199ms/step - accuracy: 0.5407 - loss: 0.8267 - val_accuracy: 0.4900 - val_loss: 0.6891\nEpoch 2/5\n\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 169ms/step - accuracy: 0.5614 - loss: 0.7396 - val_accuracy: 0.5450 - val_loss: 0.6928\nEpoch 3/5\n\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 169ms/step - accuracy: 0.6212 - loss: 0.6663 - val_accuracy: 0.5250 - val_loss: 0.6882\nEpoch 4/5\n\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 183ms/step - accuracy: 0.6459 - loss: 0.6308 - val_accuracy: 0.5500 - val_loss: 0.6880\nEpoch 5/5\n\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 172ms/step - accuracy: 0.6959 - loss: 0.5818 - val_accuracy: 0.5450 - val_loss: 0.6847\n",
     "output_type": "stream"
    },
    {
     "execution_count": 15,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x7a8f7b5d5b40>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test Accuracy:\", accuracy[1])\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T09:53:16.260196Z",
     "iopub.execute_input": "2024-03-29T09:53:16.261248Z",
     "iopub.status.idle": "2024-03-29T09:53:16.765755Z",
     "shell.execute_reply.started": "2024-03-29T09:53:16.261213Z",
     "shell.execute_reply": "2024-03-29T09:53:16.764897Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001B[1m7/7\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 54ms/step - accuracy: 0.5220 - loss: 0.6827\nTest Accuracy: 0.5450000166893005\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "violence_features_test = extract_features_from_videos(violence_path[500:510], len(violence_path[500:510]), pretrained_model)\n",
    "non_violence_features_test = extract_features_from_videos(nonviolence_path[500:510], len(nonviolence_path[500:510]), pretrained_model)\n",
    "\n",
    "# Reshape the features for LSTM input\n",
    "test_violence = violence_features_test.reshape((violence_features_test.shape[0], 16, 2048))\n",
    "test_non_violence = non_violence_features_test.reshape((non_violence_features_test.shape[0], 16, 2048))\n",
    "\n",
    "# Define class names\n",
    "class_names = ['violence', 'non_violence']\n",
    "\n",
    "# Predictions for test videos\n",
    "predicted_non_violence = [class_names[1] if i > 0.5 else class_names[0] for i in model.predict(test_non_violence)]\n",
    "predicted_violence = [class_names[1] if i > 0.5 else class_names[0] for i in model.predict(test_violence)]\n",
    "\n",
    "print(\"Predicted labels for non-violence videos:\", predicted_non_violence)\n",
    "print(\"Predicted labels for violence videos:\", predicted_violence)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T09:53:53.782521Z",
     "iopub.execute_input": "2024-03-29T09:53:53.783149Z",
     "iopub.status.idle": "2024-03-29T09:55:15.344978Z",
     "shell.execute_reply.started": "2024-03-29T09:53:53.783115Z",
     "shell.execute_reply": "2024-03-29T09:55:15.343654Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:42<00:00,  4.30s/it]\n100%|██████████| 10/10 [00:37<00:00,  3.71s/it]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 670ms/step\n\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step\nPredicted labels for non-violence videos: ['non_violence', 'violence', 'violence', 'non_violence', 'non_violence', 'non_violence', 'non_violence', 'non_violence', 'violence', 'violence']\nPredicted labels for violence videos: ['violence', 'violence', 'violence', 'violence', 'non_violence', 'non_violence', 'violence', 'non_violence', 'violence', 'non_violence']\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def classify_video(video_path, model, pretrained_model):\n",
    "    \"\"\"\n",
    "    Classifies a single video as either 'violence' or 'non_violence'.\n",
    "    \n",
    "    Args:\n",
    "    - video_path (str): The path to the video to be classified.\n",
    "    - model: The trained LSTM model for classification.\n",
    "    - pretrained_model: The pre-trained feature extraction model used for preparing the video data.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The predicted label for the video ('violence' or 'non_violence').\n",
    "    \"\"\"\n",
    "    # Extract features from the given video\n",
    "    video_features = extract_features_from_videos([video_path], 1, pretrained_model)\n",
    "    \n",
    "    # Reshape the features for LSTM input\n",
    "    video_features_reshaped = video_features.reshape((1, 16, 2048))  # Assuming the model expects inputs of shape (batch_size, 16, 2048)\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction = model.predict(video_features_reshaped)\n",
    "    predicted_label = 'non_violence' if prediction > 0.5 else 'violence'\n",
    "    \n",
    "    # Return the predicted label\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage:\n",
    "# video_path = 'path/to/your/video.mp4'\n",
    "label = \n",
    "print(f\"The video is predicted to be: {label}\")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:07:25.745042Z",
     "iopub.execute_input": "2024-03-29T10:07:25.745504Z",
     "iopub.status.idle": "2024-03-29T10:07:30.673670Z",
     "shell.execute_reply.started": "2024-03-29T10:07:25.745468Z",
     "shell.execute_reply": "2024-03-29T10:07:30.672678Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 1/1 [00:04<00:00,  4.24s/it]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 624ms/step\nThe video is predicted to be: non_violence\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "classify_video('/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/NV_1000.mp4', model, pretrained_model)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:07:59.815691Z",
     "iopub.execute_input": "2024-03-29T10:07:59.816107Z",
     "iopub.status.idle": "2024-03-29T10:08:04.079442Z",
     "shell.execute_reply.started": "2024-03-29T10:07:59.816076Z",
     "shell.execute_reply": "2024-03-29T10:08:04.078391Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 1/1 [00:04<00:00,  4.17s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    },
    {
     "execution_count": 19,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'non_violence'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
